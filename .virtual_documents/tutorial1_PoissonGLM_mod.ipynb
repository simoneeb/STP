





import numpy as np
import matplotlib.pyplot as plt
from scipy.io import loadmat
import pickle
import h5py
import imageio as iio
import pandas as pd


### ====  1. Load the raw data ============

experiment_name = 'MR-0609'

# load raw spikes
# #load events
# fp = f'/Users/simone/Documents/Experiments/Spatiotemporal_tuning_curves/{experiment_name}/sync/event_list_{experiment_name}_.csv'
# events = pd.read_csv(fp)
# evts = events[events['protocol_name']== 'checkerboard']


# # load spikes
# fpspikes = f'/Users/simone/Documents/Experiments/Spatiotemporal_tuning_curves/STA/{experiment_name}/spikes_checkerboard_nd4.txt'
# all_spike_times = np.loadtxt(fpspikes)


# load spike counts
fpspikes = f'/Users/simone/Documents/Experiments/Spatiotemporal_tuning_curves/STA/{experiment_name}/spikes_count_checkerboard_nd4.txt'  # imac
fpspikes = f'/home/sebert/Documents/Experiments/Spatiotemporal_tuning_curves/STA/{experiment_name}/spikes_count_checkerboard_nd4.txt'   # hesse


all_spike_counts = np.loadtxt(fpspikes)
nb_cells = all_spike_counts.shape[0]
nb_bins = all_spike_counts.shape[1]




# load stimulus
fpstim  = f'/Users/simone/Documents/Experiments/Spatiotemporal_tuning_curves/stimuli/{experiment_name}_checker/checkerboard_nd4_stim.mat'  # imac
fpstim  = f'/home/sebert/Documents/Experiments/Spatiotemporal_tuning_curves/stimuli/{experiment_name}_checker/checkerboard_nd4_stim.mat'   # hesse

stim =h5py.File(fpstim, 'r')
nb_frames = int(len(stim['stim'])/2)
nb_checks = stim[stim['stim'][0][0]][()].shape[0]

framerate = 30.
dt_stim = 1/framerate
time = np.arange(0,nb_frames)*dt_stim


stimmat = np.zeros((nb_frames, nb_checks,nb_checks))
for i in range(int(nb_frames/2)):
    frame = stim[stim['stim'][i][0]][()]
    stimmat[i,:,:] = frame

#stimmat2 = np.rot90(np.fliplr(stimmat))
stimmat = np.rot90(np.fliplr(stimmat), axes=(1,2),k = 3)
stimmat = stimmat[:int(len(stimmat)/2)]
#stimshape = (stimmat.shape[1],stimmat.shape[2])


### ==== 2. Bin the spike train ===== 

# For now we will assume we want to use the same time bin size as the time
# bins used for the stimulus. Later, though, we'll wish to vary this.


c = 5
spike_count = all_spike_counts[c,:]
num_spikes = np.sum(spike_count)

spikes_bin_centers = np.arange(nb_bins+1)*dt_stim

### Replot the responses we'll putting into our regression as counts
plt.clf()
plt.figure(figsize=[12,8])
plt.stem(time,spike_count)
plt.title('binned spike counts')
plt.ylabel('spike count')
plt.xlabel('time (s)')
plt.xlim(400,410)
plt.show()


### ==== 3. Build the design matrix: slow version ======
# This is a necessary step before we can fit the model: assemble a matrix
# that contains the relevant regressors for each time bin of the response,
# known as a design matrix.  Each row of this matrix contains the relevant
# stimulus chunk for predicting the spike count at a given time bin

# Set the number of time bins of stimulus to use for predicting spikes
ntfilt = 12   # Try varying this, to see how performance changes!

# Build the design matrix: Slow version

def design_matrix_2D(stimulus,td=12):
    
    stimulus_timesteps = stimulus.shape[0]+td
    preceeding_stimulus_length = stimulus.shape[1]*stimulus.shape[2]*td
    
    stimulus_padded = np.concatenate((np.zeros((td,stimulus.shape[1],stimulus.shape[2])),stimulus))
    X = np.zeros((stimulus_timesteps,preceeding_stimulus_length))
    
    for t in range(stimulus_timesteps):
        if t < td:
            continue
        else:
            X[t,:] = stimulus_padded[t-td:t,:,:].ravel()
        
    return X[td:,:]

design_mat = design_matrix_2D(stimmat,ntfilt)


# design_mat = np.zeros((num_time_bins,ntfilt))
# for j in np.arange(num_time_bins):
#     design_mat[j] = padded_stim[j:j+ntfilt] # grab last 'nkt' bins of stmiulus and insert into this row
    

# Notice it has a structure where every row is a shifted copy of the row
# above, which comes from the fact that for each time bin of response,
# we're grabbing the preceding 'nkt' bins of stimulus as predictor


# Let's visualize a small part of the design matrix just to see it
plt.clf()
plt.figure(figsize=[12,8])
plt.imshow(design_mat[:50], aspect='auto', interpolation='nearest')
plt.xlabel('lags before spike time')
plt.ylabel('time bin of response')
plt.colorbar()
plt.show()



def separate_components(sta):
    """Separate space and time components."""

    time_width, space_height, space_width = sta.shape[0], sta.shape[1], sta.shape[2]
    rf_shape = (time_width, space_height * space_width)
    rf = np.reshape(sta, rf_shape)

    # # Remove the median.
    # rf_median = np.median(rf)
    # rf = rf - rf_median

    u, s, vh = np.linalg.svd(rf, full_matrices=False)

    time_rf_shape = (time_width,)
    time_rf = np.reshape(u[:, 1], time_rf_shape)  # TODO why 1 instead of 0?
    space_rf_shape = (space_height, space_width)
    space_rf = np.reshape(vh[1, :], space_rf_shape)  # TODO understand why 1 instead of 0?

    # Determine the cell polarity
    if np.abs(np.max(rf) - np.median(rf)) >= np.abs(np.min(rf) - np.median(rf)):
        rf_polarity = 'ON'
    else:
        rf_polarity = 'OFF'
        
    # Determine the spatial RF polarity
    if np.abs(np.max(space_rf) - np.median(space_rf) >= np.abs(np.min(space_rf) - np.median(space_rf))):
        space_rf_polarity = 'ON'
    else:
        space_rf_polarity = 'OFF'
        
    # Determine the temporal RF polarity
    if np.abs(np.max(time_rf) - np.median(time_rf) >= np.abs(np.min(time_rf) - np.median(time_rf))):
        time_rf_polarity = 'ON'
    else:
        time_rf_polarity = 'OFF'
        
    # Reverse components (if necessary).
    if rf_polarity != space_rf_polarity:
        space_rf = - space_rf
        
    if rf_polarity != time_rf_polarity:
        time_rf = - time_rf

    return time_rf, space_rf



### === 4. Compute and visualize the spike-triggered average (STA) ====

# When the stimulus is Gaussian white noise, the STA provides an unbiased
# estimator for the filter in a GLM / LNP model (as long as the nonlinearity
# results in an STA whose expectation is not zero; feel free 
# to ignore this parenthetical remark if you're not interested in technical
# details. It just means that if the nonlinearity is symmetric, 
# eg. x^2, then this condition won't hold, and the STA won't be useful).

# In many cases it's useful to visualize the STA (even if your stimuli are
# not white noise), just because if we don't see any kind of structure then
# this may indicate that we have a problem (e.g., a mismatch between the
# design matrix and binned spike counts.

### It's extremely easy to compute the STA now that we have the design matrix
sta = (design_mat.T @ spike_count)/np.sum(spike_count)
sta_3d = sta.reshape((ntfilt,nb_checks,nb_checks))
#stas = [design_mats[rep].T @ -spikes_binned[rep] for rep in range(nb_repetitions)]
trf,srf = separate_components(sta_3d)
#tp_tot, sp_tot = separate_components(sta)
### Plot it
ttk = np.arange(-ntfilt+1,1)*dt_stim # time bins for STA (in seconds)
plt.clf()

plt.figure(figsize=[12,8])

plt.plot(ttk,ttk*0, 'k--')
# for x in range(nb_checks):
#     for y in range(nb_checks):
#         plt.plot(ttk, sta_3d[:,x,y], alpha = .3)
# plt.plot(ttk, temporal, 'go-')
# plt.plot(ttk, tp_tot, 'bo-')
plt.plot(ttk, trf, 'bo-')

plt.title('STA')
plt.xlabel('time before spike (s)')
plt.xlim([ttk[0],ttk[-1]])
plt.show()

# If you're still using cell #1, this should look like a biphasic filter
# with a negative lobe just prior to the spike time.

# (By contrast, if this looks like garbage then it's a good chance we did something wrong!)


sta.shape,sta_3d.shape


### ===== 4b. whitened STA (ML fit to filter for linear-Gaussian GLM) ======

# If the stimuli are non-white, then the STA is generally a biased
# estimator for the linear filter. In this case we may wish to compute the
# "whitened" STA, which is also the maximum-likelihood estimator for the filter of a 
# GLM with "identity" nonlinearity and Gaussian noise (also known as
# least-squares regression).

# If the stimuli have correlations this ML estimate may look like garbage
# (more on this later when we come to "regularization").  But for this
# dataset the stimuli are white, so we don't (in general) expect a big
# difference from the STA.  (This is because the whitening matrix
# (Xdsng.T * Xdsgn)^{-1} is close to a scaled version of the identity.)

# from numpy.linalg import inv, norm
# ### whitened STA
# #wsta = inv(design_mat[0].T @ design_mat[0]) @ sta[0] * num_spikes
# wstas = [ (inv(design_mats[rep].T @ design_mats[rep]) @ stas[rep])*np.sum(spikes_binned[rep]) for rep in range(nb_repetitions)]

# # this is just the least-squares regression formula!

# ### Let's plot them both (rescaled as unit vectors so we can see differences in their shape).
# plt.clf()
# plt.figure(figsize=[12,8])
# plt.plot(ttk,ttk*0, 'k--')
# plt.plot(ttk, sta/norm(sta), 'bo-', label="STA")
# plt.plot(ttk, wsta/norm(wsta), 'ro-', label="wSTA")
# plt.legend()
# plt.title('STA and whitened STA')
# plt.xlabel('time before spike (s)')
# plt.xlim([ttk[0],ttk[-1]])
# plt.show()





### ===== 4c. Predicting spikes with a linear-Gaussian GLM ======

# The whitened STA can actually be used to predict spikes because it
# corresponds to a proper estimate of the model parameters (i.e., for a
# Gaussian GLM). Let's inspect this prediction

sppred_lgGLM = design_mat @ sta  # predicted spikes from linear-Gaussian GLM

# Let's see how good this "prediction" is
# (Prediction in quotes because we are (for now) looking at the performance
# on training data, not test data... so it isn't really aprediction!)

### Plot real spike train and prediction
plt.clf()
plt.figure(figsize=[12,8])
markerline,_,_ = plt.stem(spike_count, linefmt='b-', basefmt='k-', label="spike ct")
plt.setp(markerline, 'markerfacecolor', 'none')
plt.setp(markerline, 'markeredgecolor', 'blue')
plt.plot(sppred_lgGLM/sppred_lgGLM.max(), color='red', linewidth=2, label="lgGLM")
plt.title('linear-Gaussian GLM: spike count prediction')
plt.ylabel('spike count'); plt.xlabel('time (s)')
#plt.xlim(5550,6050)
plt.legend()
plt.show()


import statsmodels.api as sm




design_mat_offset = np.hstack((np.ones((nb_bins,1)), design_mat))     # just add a column of ones
print(design_mat.shape, design_mat_offset.shape)



### This is super-easy if we rely on built-in GLM fitting code, takes 2-3 min
glm_poisson_exp = sm.GLM(endog=spike_count, exog=design_mat_offset,
                         family=sm.families.Poisson())


pGLM_results = glm_poisson_exp.fit(max_iter=100, tol=1e-6, tol_criterion='params') # takes forever


# pGLM_const = glm_poisson_exp[-1].fit_['beta0'] # constant ("dc term)")
pGLM_const = pGLM_results.params[0]
pGLM_filt = pGLM_results.params[1:] # stimulus filter

pGLM_filt_3d = pGLM_filt.reshape((ntfilt,nb_checks,nb_checks))
#stas = [design_mats[rep].T @ -spikes_binned[rep] for rep in range(nb_repetitions)]
pGLM_trf,pGLM_srf = separate_components(pGLM_filt_3d)


### ======  5. Poisson GLM ====================

# Let's finally move on to the LNP / Poisson GLM!

# Package available for download from
# https://www.statsmodels.org/stable/install.html






# The 'GLM' function can fit a GLM for us. Here we have specified that
# we want the noise model to be Poisson. The default setting for the link
# function (the inverse of the nonlinearity) is 'log', so default
# nonlinearity is 'exp').  

### Compute predicted spike rate on training data
rate_pred_pGLM = np.exp(pGLM_const + design_mat @ pGLM_filt)
# equivalent to if we had just written np.exp(design_mat_offset @ glm_poisson_exp)/dt_stim


pGLM_filt_3d.shape


# pGLM_filt_3d = pGLM_filt.reshape((ntfilt,stimshape[0],stimshape[1]))
# #stas = [design_mats[rep].T @ -spikes_binned[rep] for rep in range(nb_repetitions)]
# pGLM_trf,pGLM_srf = separate_components(pGLM_filt_3d)


### ===== 5b. Make plots showing and spike rate predictions ======

plt.close()

fig, (ax1,ax2) = plt.subplots(2)
fig.set_size_inches(12,8)
ax1.plot(ttk,ttk*0, 'k--')
#ax1.plot(ttk, wsta_offset/norm(wsta_offset), 'o-', label='lin-gauss GLM filt', c='orange')
ax1.plot(ttk, pGLM_trf, 'o-', label='poisson GLM filt', c='r')
ax1.plot(ttk, trf, 'bo-', label= 'STA for linear-gaussian GLM')
ax1.legend(loc = 'upper left')
ax1.set_title('(normalized) linear-Gaussian and Poisson GLM filter estimates')
ax1.set_xlabel('time before spike (s)')
ax1.set_xlim([ttk[0], ttk[-1]])

markerline,stemlines,baseline = plt.stem(time,spike_count, label="spike count", linefmt='b-', basefmt='b-')
plt.setp(markerline, 'markerfacecolor', 'none')
plt.setp(stemlines, color='b', linewidth=.5)
plt.setp(baseline, color='b', linewidth=.5)
ax2.plot(time,sppred_lgGLM, label="lin-gauss GLM", c='b')
ax2.plot(time,rate_pred_pGLM, label="exp-poisson GLM", c='r') 
ax2.set_title('spike count / rate predictions')
ax2.set_ylabel('spike count / bin'); plt.xlabel('time (s)')
ax2.set_xlim(400,410)
ax2.legend(loc='upper right')
plt.tight_layout()

# Note the rate prediction here is in units of spikes/bin. If we wanted
# spikes/sec, we could divide it by bin size dt_stim.


### ===== 6. Non-parametric estimate of the nonlinearity =====

# The above fitting code assumes a GLM with an exponential nonlinearity
# (i.e., governing the mapping from filter output to instantaneous spike
# rate). We might wish to examine the adequacy of that assumption and make
# a "nonparametric" estimate of the nonlinearity using a more flexible
# class of functions.

# Let's use the family of piece-wise constant functions, which results in a
# very simple estimation procedure:
# 1. Bin the filter outputs
# 2. In each bin, compute the fraction of stimuli elicted spikes

from scipy.interpolate import interp1d

# number of bins for parametrizing the nonlinearity f. (Try varying this!) 
num_fbins = 25

# compute filtered stimulus
raw_filter_output = pGLM_const + design_mat @ pGLM_filt

# bin filter output and get bin index for each filtered stimulus
counts,bin_edges = np.histogram(raw_filter_output,num_fbins);
bin_idx = np.digitize(raw_filter_output, bins=bin_edges) - 1
fx = bin_edges[:-1]+(bin_edges[1]-bin_edges[0])/2 # use bin centers for x positions

# now compute mean spike count in each bin
fy = np.zeros(num_fbins) # y values for nonlinearity
for jj in np.arange(num_fbins):
    fy[jj] = np.mean(spike_count[bin_idx==jj])
fy = fy/dt_stim # divide by bin size to get units of sp/s;

# Scipy has a handy class that embeds these approximations into an interpolating function
fnlin = interp1d(fx,fy,kind='nearest', bounds_error=False, fill_value='extrapolate')

# Make plots
plt.close()

fig, (ax1,ax2) = plt.subplots(2) # Plot exponential and nonparametric nonlinearity estimate
fig.set_size_inches(12,8)
ax1.bar(fx,counts)
ax1.set_ylabel('count')
ax1.set_title('histogram of filter outputs')

xx = np.arange(bin_edges[0], bin_edges[-1]+.01, step=.01)
ax2.plot(xx,np.exp(xx)/dt_stim, label='exponential f', c='b')
ax2.plot(xx,fnlin(xx), label='nonparametric f', c='orange')
ax2.set_xlabel('filter output')
ax2.set_ylabel('rate (sp/s)')
ax2.legend(loc='upper left')
ax2.set_title('nonlinearity')
plt.tight_layout()

# What do you think of the exponential fit? Does this look like a good
# approximation to the nonparametric estimate of the nonlinearity?  Can you
# propose a better parametric nonlinearity to use instead?  

# Advanced exercise: write your own log-likelihood function that allows you
# to jointly optimize log-likelihood for the filter parameters and
# nonlinearity.  (By contrast, here we have optimized filter params under
# exponential nonlinearity and THEN fit the nonlinearity using these fixed
# filter parameters).  We could, for example, iteratively climb the
# log-likelihood as a function of filter params and nonlinearity params;
# this is a method known as "coordinate ascent").


### ======= 7. Quantifying performance: log-likelihood =======

# Lastly, compute log-likelihood for the Poisson GLMs we've used so far and
# compare performance.

# LOG-LIKELIHOOD (this is what glmfit maximizes when fitting the GLM):
# --------------
# Let s be the spike count in a bin and r is the predicted spike rate
# (known as "conditional intensity") in units of spikes/bin, then we have:   
#
#        Poisson likelihood:      P(s|r) = r^s/s! exp(-r)  
#     giving log-likelihood:  log P(s|r) =  s log r - r   
#
# (where we have ignored the -log s! term because it is independent of the
# parameters). The total log-likelihood is the summed log-likelihood over
# time bins in the experiment.

# 1. for GLM with exponential nonlinearity
rate_pred_pGLM = np.exp(pGLM_const + design_mat@pGLM_filt)# rate under exp nonlinearity
LL_expGLM = spike_count.T @ np.log(rate_pred_pGLM) - np.sum(rate_pred_pGLM)

# 2. for GLM with non-parametric nonlinearity
rate_pred_pGLMnp = dt_stim * fnlin(pGLM_const + design_mat @ pGLM_filt) # rate under nonpar nonlinearity
LL_npGLM = spike_count[spike_count>0].T @ np.log(rate_pred_pGLMnp[spike_count>0]) - np.sum(rate_pred_pGLMnp)

# Now compute the rate under "homogeneous" Poisson model that assumes a
# constant firing rate with the correct mean spike count.
rate_pred_const = num_spikes/nb_frames # mean number of spikes / bin
LL0 = num_spikes*np.log(rate_pred_const) - nb_frames*rate_pred_const

# Single-spike information:
# ------------------------
# The difference of the loglikelihood and homogeneous-Poisson
# loglikelihood, normalized by the number of spikes, gives us an intuitive
# way to compare log-likelihoods in units of bits / spike.  This is a
# quantity known as the (empirical) single-spike information.
# [See Brenner et al, "Synergy in a Neural Code", Neural Comp 2000].
# You can think of this as the number of bits (number of yes/no questions
# that we can answer) about the times of spikes when we know the spike rate
# output by the model, compared to when we only know the (constant) mean
# spike rate. 

SSinfo_expGLM = (LL_expGLM - LL0)/num_spikes/np.log(2)
SSinfo_npGLM = (LL_npGLM - LL0)/num_spikes/np.log(2)
# (if we don't divide by log 2 we get it in nats)

print('\n empirical single-spike information:\n ---------------------- ')
print(f'exp-GLM: {SSinfo_expGLM:.2f} bits/sp')
print(f' np-GLM: {SSinfo_npGLM:.2f} bits/sp')

# Let's plot the rate predictions for the two models 
# --------------------------------------------------
plt.clf()
plt.figure(figsize=[12,8])
markerline,stemlines,baseline = plt.stem(time,spike_count, label="spike count", linefmt='b-', basefmt='b-')
plt.setp(markerline, 'markerfacecolor', 'none')
plt.setp(stemlines, color='b', linewidth=.5)
plt.setp(baseline, color='b', linewidth=.5)
plt.plot(time,rate_pred_pGLM, label='exp-GLM', c='r')
plt.plot(time,rate_pred_pGLMnp, label='np-GLM', c='orange')
plt.title('rate predictions')
plt.ylabel('spikes / bin')
plt.xlabel('time (s)')
plt.xlim(400,410)
plt.legend()
plt.tight_layout()


### ========== 8. Quantifying performance: AIC =============

# Akaike information criterion (AIC) is a method for model comparison that
# uses the maximum likelihood, penalized by the number of parameters.
# (This allows us to compensate for the fact that models with more
# parameters can in general achieve higher log-likelihood. AIC determines
# how big this tradeoff should be in terms of the quantity:
#        AIC = - 2*log-likelihood + 2 * number-of-parameters
# The model with lower AIC is 
# their likelihood (at the ML estimate), penalized by the number of parameters  

AIC_expGLM = -2*LL_expGLM + 2*(1+ntfilt);
AIC_npGLM = -2*LL_npGLM + 2*(1+ntfilt+num_fbins)

print('\n AIC comparison:\n ---------------------- ')
print(f'exp-GLM: {AIC_expGLM:.1f}')
print(f' np-GLM: {AIC_npGLM:.1f}')
print(f'\nAIC diff (exp-np)= {AIC_expGLM-AIC_npGLM:.2f}')
if AIC_expGLM < AIC_npGLM:
    print('AIC supports exponential-nonlinearity!')
else:
    print('AIC supports nonparametric nonlinearity!')
    # (despite its greater number of parameters)

# Caveat: technically the AIC should be using the maximum of the likelihood
# for a given model.  Here we actually have an underestimate of the
# log-likelihood for the non-parameteric nonlinearity GLM because
# because we left the filter parameters unchanged from the exponential-GLM.
# So a proper AIC comparison (i.e., if we'd achieved a true ML fit) would
# favor the non-parametric nonlinearity GLM even more!

# Exercise: go back and increase 'nfbins', the number of parameters (bins)
# governing the nonparametric nonlinearity. If you increase it enough, you
# should be able to flip the outcome so exponential nonlinearity wins.

# (Note: in the third tutorial we'll use cross-validation to properly
# evaluate the goodness of the fit of the models, e.g., allowing us to
# decide how many bins of stimulus history or how many bins to use for the
# non-parametric nonlinearity, or how to set regularization
# hyperparameters. The basic idea is to split data into training and test
# sets.  Fit the parameters on the training set, and compare models by
# evaluating log-likelihood on test set.)


# predict STP stim


